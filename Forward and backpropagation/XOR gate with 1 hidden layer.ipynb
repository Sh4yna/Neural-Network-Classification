{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "1bb100ff-e90f-4456-b88c-a79d0d711374",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "1647c287-b2b0-48ab-82c9-da8625faf89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    def __init__(self):\n",
    "        self.inputs=np.array([[1,1,0,0],[0,1,1,0],[1,1,1,1]])\n",
    "        self.z=np.array([[1],[0],[1],[0]])\n",
    "        self.weights_bias1=np.array([[20,20,-10],[-20,-20,30]]) #first layer\n",
    "        self.weights_bias2=np.array([[20,20,-30]]) #second layer\n",
    "        self.alpha=0.5\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "        \n",
    "    def sigmoid_der(self, x):\n",
    "        return (np.exp(-x)/((1+np.exp(-x))**2))\n",
    "        \n",
    "    def forward_prop(self):\n",
    "        self.z1=np.matmul(self.weights_bias1, self.inputs)\n",
    "        self.a1=self.sigmoid(self.z1)  #activations of first layer\n",
    "        self.a1=np.vstack((self.a1,np.array([[1,1,1,1]])))\n",
    "        self.z2=np.matmul(self.weights_bias2, self.a1).T\n",
    "        self.a2=self.sigmoid(self.z2)  #final output\n",
    "        \n",
    "    def cost(self):\n",
    "        self.forward_prop()\n",
    "        return ((((self.a2-self.z)**2).sum(0))/4)\n",
    "\n",
    "    def backprop(self):\n",
    "        for i in range(10000):\n",
    "            self.forward_prop()\n",
    "            c=self.a2-self.z   #derivative of cost wrt final activation\n",
    "            \n",
    "            s=self.sigmoid_der(self.z2)    #derivative of final activation wrt the weighted sum obtained from the hidden layer\n",
    "            \n",
    "            self.weights_bias2=self.weights_bias2-((self.alpha*np.matmul(self.a1,c*s)).T)  #derivative of weighted sum wrt the weights and biases of hidden layer\n",
    "            \n",
    "            delta_a1=np.matmul(c*s, self.weights_bias2[:,0:2])     #derivative of weighted sum wrt the activations of hidden layer\n",
    "            \n",
    "            delta_a1=delta_a1.sum(0)/4          #average derivative for each activation over the 4 inputs\n",
    "            \n",
    "            s2=self.sigmoid_der(self.z1)        #derivative of the activations of hidden layer wrt weighted sum from input layer\n",
    "            s2=s2.T\n",
    "            \n",
    "            self.weights_bias1=self.weights_bias1-self.alpha*((np.matmul(self.inputs, s2))*(delta_a1)).T   #derivative of the weighted sum from input layer wrt its weights and biases\n",
    "\n",
    "    def output(self, a, b):\n",
    "        self.z1=np.matmul(self.weights_bias1, np.array([[a],[b],[1]]))\n",
    "        self.a1=self.sigmoid(self.z1)\n",
    "        self.a1=np.vstack((self.a1,np.array([[1]])))\n",
    "        self.z2=np.matmul(self.weights_bias2, self.a1).T\n",
    "        self.a2=self.sigmoid(self.z2)\n",
    "        print(self.a2)\n",
    "        print(self.weights_bias1)\n",
    "        print(self.weights_bias2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "73e2a4e0-95d5-491b-82e7-67938377d8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN=NeuralNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "90615707-5634-4e8a-9b5f-fcfad4249e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN.forward_prop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "31bb81d2-cfc9-48a3-a42c-6531aa3daef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN.backprop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "5514ec12-5a93-4a01-a1bd-9e297da76612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.06656708e-09])"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN.cost()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "4b7310da-a7f4-40b2-b9d6-af498bb2131c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99583733]]\n",
      "[[ 20.00043261  20.00043261  -9.99870151]\n",
      " [-19.99954782 -19.99954782  30.00067812]]\n",
      "[[ 21.82324984  11.83236993 -28.17666704]]\n"
     ]
    }
   ],
   "source": [
    "NN.output(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807ccc43-c65e-4cee-9783-3d7d74e9611d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1b444a-9de2-48f2-9a89-9ceebc27430b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
